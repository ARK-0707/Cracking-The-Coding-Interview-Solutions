# 16.25 LRU Cache

## Description

Design and build a `"least recently used"` cache, which evicts the least recently used item. The cache should map from `keys to values` (allowing you to insert and retrieve a value associated with a particular key) and be initialized with a max size. When it is full, it should evict the least recently used item.

**Example**

```python
cache = LRUCache(2)
cache.put(1, 1)           # Cache is {1=1}
cache.put(2, 2)           # Cache is {1=1, 2=2}
cache.get(1)              # Returns 1, Cache is {2=2, 1=1}
cache.put(3, 3)           # Evicts key 2, Cache is {1=1, 3=3}
cache.get(2)              # Returns -1 (not found)
cache.get(3)              # Returns 3
```

## Explanation

The `LRUCache` uses a combination of:

* **HashMap** for O(1) access to nodes.
* **Doubly Linked List** to maintain access order from most to least recently used.

### Components

* `Node`: Represents a key-value pair in the cache.
* `map`: Dictionary to store key-node mappings.
* `head/tail`: Dummy nodes to simplify list manipulation (head = MRU, tail = LRU).
* `_remove(node)`: Removes a node from the list.
* `_add_to_front(node)`: Adds a node right after the head.

### Complexity

#### Time Complexity:

* `get(key)` – O(1): Direct lookup and node movement.
* `put(key, value)` – O(1): Insert/update and possible eviction.

#### Space Complexity:

* O(n): Stores up to `n` key-node pairs in the map and doubly linked list.

## Python Code

```python
class Node:
    def __init__(self, key, value):
        self.key = key
        self.value = value
        self.prev = None
        self.next = None


class LRUCache:
    def __init__(self, capacity):
        self.capacity = capacity
        self.map = {}
        self.head = Node(0, 0)  # Dummy head
        self.tail = Node(0, 0)  # Dummy tail
        self.head.next = self.tail
        self.tail.prev = self.head

    def _remove(self, node):
        prev, nxt = node.prev, node.next
        prev.next = nxt
        nxt.prev = prev

    def _add_to_front(self, node):
        node.prev = self.head
        node.next = self.head.next
        self.head.next.prev = node
        self.head.next = node

    def get(self, key):
        if key in self.map:
            node = self.map[key]
            self._remove(node)
            self._add_to_front(node)
            return node.value
        return -1

    def put(self, key, value):
        if key in self.map:
            self._remove(self.map[key])
        node = Node(key, value)
        self._add_to_front(node)
        self.map[key] = node

        if len(self.map) > self.capacity:
            lru = self.tail.prev
            self._remove(lru)
            del self.map[lru.key]


# Example usage
cache = LRUCache(2)
cache.put(1, 1)  # Cache is {1=1}
cache.put(2, 2)  # Cache is {1=1, 2=2}
print(cache.get(1))  # Returns 1
cache.put(3, 3)  # Evicts key 2, Cache is {1=1, 3=3}
print(cache.get(2))  # Returns -1
print(cache.get(3))  # Returns 3
```

By combining a hashmap with a doubly linked list, this implementation provides efficient O(1) access and updates, making it a suitable choice for real-time caching systems.
